// Jest Snapshot v1, https://goo.gl/fbAQLP

exports[`Elasticsearch 1`] = `
"apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  labels:
    app: elasticsearch-master
  name: elasticsearch-master-pdb
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app: elasticsearch-master
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: elasticsearch-master
    chart: elasticsearch
    heritage: Helm
    release: elasticsearch-c80e37ed
  name: elasticsearch-master
spec:
  ports:
    - name: http
      port: 9200
      protocol: TCP
    - name: transport
      port: 9300
      protocol: TCP
  selector:
    app: elasticsearch-master
    chart: elasticsearch
    heritage: Helm
    release: elasticsearch-c80e37ed
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: \\"true\\"
  labels:
    app: elasticsearch-master
    chart: elasticsearch
    heritage: Helm
    release: elasticsearch-c80e37ed
  name: elasticsearch-master-headless
spec:
  clusterIP: None
  ports:
    - name: http
      port: 9200
    - name: transport
      port: 9300
  publishNotReadyAddresses: true
  selector:
    app: elasticsearch-master
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  annotations:
    esMajorVersion: \\"7\\"
  labels:
    app: elasticsearch-master
    chart: elasticsearch
    heritage: Helm
    release: elasticsearch-c80e37ed
  name: elasticsearch-master
spec:
  podManagementPolicy: Parallel
  replicas: 1
  selector:
    matchLabels:
      app: elasticsearch-master
  serviceName: elasticsearch-master-headless
  template:
    metadata:
      labels:
        app: elasticsearch-master
        chart: elasticsearch
        heritage: Helm
        release: elasticsearch-c80e37ed
      name: elasticsearch-master
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: app
                    operator: In
                    values:
                      - elasticsearch-master
              topologyKey: kubernetes.io/hostname
      containers:
        - env:
            - name: node.name
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: cluster.initial_master_nodes
              value: elasticsearch-master-0,
            - name: discovery.seed_hosts
              value: elasticsearch-master-headless
            - name: cluster.name
              value: elasticsearch
            - name: network.host
              value: 0.0.0.0
            - name: ES_JAVA_OPTS
              value: -Xmx1g -Xms1g
            - name: node.data
              value: \\"true\\"
            - name: node.ingest
              value: \\"true\\"
            - name: node.master
              value: \\"true\\"
          image: docker.elastic.co/elasticsearch/elasticsearch:7.6.1
          imagePullPolicy: IfNotPresent
          name: elasticsearch
          ports:
            - containerPort: 9200
              name: http
            - containerPort: 9300
              name: transport
          readinessProbe:
            exec:
              command:
                - sh
                - -c
                - |
                  #!/usr/bin/env bash -e
                  # If the node is starting up wait for the cluster to be ready (request params: 'wait_for_status=green&timeout=1s' )
                  # Once it has started only check that the node itself is responding
                  START_FILE=/tmp/.es_start_file

                  http () {
                      local path=\\"\${1}\\"
                      if [ -n \\"\${ELASTIC_USERNAME}\\" ] && [ -n \\"\${ELASTIC_PASSWORD}\\" ]; then
                        BASIC_AUTH=\\"-u \${ELASTIC_USERNAME}:\${ELASTIC_PASSWORD}\\"
                      else
                        BASIC_AUTH=''
                      fi
                      curl -XGET -s -k --fail \${BASIC_AUTH} http://127.0.0.1:9200\${path}
                  }

                  if [ -f \\"\${START_FILE}\\" ]; then
                      echo 'Elasticsearch is already running, lets check the node is healthy and there are master nodes available'
                      http \\"/_cluster/health?timeout=0s\\"
                  else
                      echo 'Waiting for elasticsearch cluster to become ready (request params: \\"wait_for_status=green&timeout=1s\\" )'
                      if http \\"/_cluster/health?wait_for_status=green&timeout=1s\\" ; then
                          touch \${START_FILE}
                          exit 0
                      else
                          echo 'Cluster is not yet ready (request params: \\"wait_for_status=green&timeout=1s\\" )'
                          exit 1
                      fi
                  fi
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 3
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 1000m
              memory: 2Gi
            requests:
              cpu: 1000m
              memory: 2Gi
          securityContext:
            capabilities:
              drop:
                - ALL
            runAsNonRoot: true
            runAsUser: 1000
          volumeMounts:
            - mountPath: /usr/share/elasticsearch/data
              name: elasticsearch-master
      initContainers:
        - command:
            - sysctl
            - -w
            - vm.max_map_count=262144
          image: docker.elastic.co/elasticsearch/elasticsearch:7.6.1
          imagePullPolicy: IfNotPresent
          name: configure-sysctl
          resources: {}
          securityContext:
            privileged: true
            runAsUser: 0
      securityContext:
        fsGroup: 1000
        runAsUser: 1000
      terminationGracePeriodSeconds: 120
  updateStrategy:
    type: RollingUpdate
  volumeClaimTemplates:
    - metadata:
        name: elasticsearch-master
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 30Gi
"
`;

exports[`Kapp Rules 1`] = `
"apiVersion: kapp.k14s.io/v1alpha1
kind: Config
metadata:
  name: kapp-rules-c852dca8
rebaseRules:
  - path:
      - metadata
      - annotations
      - control-plane.alpha.kubernetes.io/leader
    resourceMatchers:
      - apiVersionKindMatcher:
          apiVersion: v1
          kind: PersistentVolumeClaim
    sources:
      - new
      - existing
    type: copy
"
`;

exports[`Kibana 1`] = `
"apiVersion: v1
kind: ServiceAccount
imagePullSecrets:
  - name: gcr-access-token
metadata:
  labels:
    app: kibana
  name: kibana-sa
---
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    app: kibana
    release: kibana-c8c926d5
  name: kibana-config
data:
  kibana.yml: |
    
    server:
      basePath: /kibana
      rewriteBasePath: true
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: kibana
    heritage: Helm
    release: kibana-c8c926d5
  name: kibana
spec:
  ports:
    - name: http
      port: 5601
      protocol: TCP
      targetPort: 5601
  selector:
    app: kibana
    release: kibana-c8c926d5
  type: ClusterIP
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: kibana
    release: kibana-c8c926d5
  name: kibana
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kibana
      release: kibana-c8c926d5
  strategy:
    type: Recreate
  template:
    metadata:
      annotations:
        configchecksum: 8647f028d3de019f20ba90b0e38a9b49670d5cf48781c19b5c568007c36d10a
      labels:
        app: kibana
        release: kibana-c8c926d5
    spec:
      containers:
        - env:
            - name: ELASTICSEARCH_HOSTS
              value: http://elasticsearch-master.default:9200
            - name: SERVER_HOST
              value: 0.0.0.0
          image: gcr.io/tensorleap/kibanimat:master-3266eae8-stable
          imagePullPolicy: IfNotPresent
          name: kibana
          ports:
            - containerPort: 5601
          readinessProbe:
            exec:
              command:
                - sh
                - -c
                - |
                  #!/usr/bin/env bash -e
                  http () {
                      local path=\\"\${1}\\"
                      set -- -XGET -s --fail

                      if [ -n \\"\${ELASTICSEARCH_USERNAME}\\" ] && [ -n \\"\${ELASTICSEARCH_PASSWORD}\\" ]; then
                        set -- \\"$@\\" -u \\"\${ELASTICSEARCH_USERNAME}:\${ELASTICSEARCH_PASSWORD}\\"
                      fi

                      STATUS=$(curl --output /dev/null --write-out \\"%{http_code}\\" -k \\"$@\\" \\"http://localhost:5601\${path}\\")
                      if [[ \\"\${STATUS}\\" -eq 200 ]]; then
                        exit 0
                      fi

                      echo \\"Error: Got HTTP code \${STATUS} but expected a 200\\"
                      exit 1
                  }

                  http \\"/kibana/app/kibana\\"
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 3
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 1000m
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 500Mi
          securityContext:
            capabilities:
              drop:
                - ALL
            runAsNonRoot: true
            runAsUser: 1000
          volumeMounts:
            - mountPath: /usr/share/kibana/config/kibana.yml
              name: kibanaconfig
              subPath: kibana.yml
      securityContext:
        fsGroup: 1000
      serviceAccount: kibana-sa
      volumes:
        - configMap:
            name: kibana-config
          name: kibanaconfig
"
`;

exports[`Minio 1`] = `
"apiVersion: v1
kind: Secret
data:
  rootPassword: Zm9vYmFyYmF6cXV4
  rootUser: Zm9vYmFyYmF6
metadata:
  labels:
    app: minio
  name: minio-secret
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app: minio
  name: minio-sa
  namespace: default
---
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    app: minio
    chart: minio-3.4.3
    heritage: Helm
    release: minio-c8cb4064
  name: minio-c8cb4064
  namespace: default
data:
  add-user: |-
    #!/bin/sh
    set -e ; # Have script exit in the event of a failed command.
    MC_CONFIG_DIR=\\"/etc/minio/mc/\\"
    MC=\\"/usr/bin/mc --insecure --config-dir \${MC_CONFIG_DIR}\\"

    # connectToMinio
    # Use a check-sleep-check loop to wait for MinIO service to be available
    connectToMinio() {
      SCHEME=$1
      ATTEMPTS=0 ; LIMIT=29 ; # Allow 30 attempts
      set -e ; # fail if we can't read the keys.
      ACCESS=$(cat /config/rootUser) ; SECRET=$(cat /config/rootPassword) ;
      set +e ; # The connections to minio are allowed to fail.
      echo \\"Connecting to MinIO server: $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT\\" ;
      MC_COMMAND=\\"\${MC} config host add myminio $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT $ACCESS $SECRET\\" ;
      $MC_COMMAND ;
      STATUS=$? ;
      until [ $STATUS = 0 ]
      do
        ATTEMPTS=\`expr $ATTEMPTS + 1\` ;
        echo \\\\\\"Failed attempts: $ATTEMPTS\\\\\\" ;
        if [ $ATTEMPTS -gt $LIMIT ]; then
          exit 1 ;
        fi ;
        sleep 2 ; # 1 second intervals between attempts
        $MC_COMMAND ;
        STATUS=$? ;
      done ;
      set -e ; # reset \`e\` as active
      return 0
    }

    # checkUserExists ($username)
    # Check if the user exists, by using the exit code of \`mc admin user info\`
    checkUserExists() {
      USER=$1
      CMD=$(\${MC} admin user info myminio $USER > /dev/null 2>&1)
      return $?
    }

    # createUser ($username, $password, $policy)
    createUser() {
      USER=$1
      PASS=$2
      POLICY=$3

      # Create the user if it does not exist
      if ! checkUserExists $USER ; then
        echo \\"Creating user '$USER'\\"
        \${MC} admin user add myminio $USER $PASS
      else
        echo \\"User '$USER' already exists.\\"
      fi


      # set policy for user
      if [ ! -z $POLICY -a $POLICY != \\" \\" ] ; then
          echo \\"Adding policy '$POLICY' for '$USER'\\"
          \${MC} admin policy set myminio $POLICY user=$USER
      else
          echo \\"User '$USER' has no policy attached.\\"
      fi
    }

    # Try connecting to MinIO instance
    scheme=http
    connectToMinio $scheme


    # Create the users
    createUser console console123 consoleAdmin
  initialize: |-
    #!/bin/sh
    set -e ; # Have script exit in the event of a failed command.
    MC_CONFIG_DIR=\\"/etc/minio/mc/\\"
    MC=\\"/usr/bin/mc --insecure --config-dir \${MC_CONFIG_DIR}\\"

    # connectToMinio
    # Use a check-sleep-check loop to wait for MinIO service to be available
    connectToMinio() {
      SCHEME=$1
      ATTEMPTS=0 ; LIMIT=29 ; # Allow 30 attempts
      set -e ; # fail if we can't read the keys.
      ACCESS=$(cat /config/rootUser) ; SECRET=$(cat /config/rootPassword) ;
      set +e ; # The connections to minio are allowed to fail.
      echo \\"Connecting to MinIO server: $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT\\" ;
      MC_COMMAND=\\"\${MC} config host add myminio $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT $ACCESS $SECRET\\" ;
      $MC_COMMAND ;
      STATUS=$? ;
      until [ $STATUS = 0 ]
      do
        ATTEMPTS=\`expr $ATTEMPTS + 1\` ;
        echo \\\\\\"Failed attempts: $ATTEMPTS\\\\\\" ;
        if [ $ATTEMPTS -gt $LIMIT ]; then
          exit 1 ;
        fi ;
        sleep 2 ; # 1 second intervals between attempts
        $MC_COMMAND ;
        STATUS=$? ;
      done ;
      set -e ; # reset \`e\` as active
      return 0
    }

    # checkBucketExists ($bucket)
    # Check if the bucket exists, by using the exit code of \`mc ls\`
    checkBucketExists() {
      BUCKET=$1
      CMD=$(\${MC} ls myminio/$BUCKET > /dev/null 2>&1)
      return $?
    }

    # createBucket ($bucket, $policy, $purge)
    # Ensure bucket exists, purging if asked to
    createBucket() {
      BUCKET=$1
      POLICY=$2
      PURGE=$3
      VERSIONING=$4

      # Purge the bucket, if set & exists
      # Since PURGE is user input, check explicitly for \`true\`
      if [ $PURGE = true ]; then
        if checkBucketExists $BUCKET ; then
          echo \\"Purging bucket '$BUCKET'.\\"
          set +e ; # don't exit if this fails
          \${MC} rm -r --force myminio/$BUCKET
          set -e ; # reset \`e\` as active
        else
          echo \\"Bucket '$BUCKET' does not exist, skipping purge.\\"
        fi
      fi

      # Create the bucket if it does not exist
      if ! checkBucketExists $BUCKET ; then
        echo \\"Creating bucket '$BUCKET'\\"
        \${MC} mb myminio/$BUCKET
      else
        echo \\"Bucket '$BUCKET' already exists.\\"
      fi


      # set versioning for bucket
      if [ ! -z $VERSIONING ] ; then
        if [ $VERSIONING = true ] ; then
            echo \\"Enabling versioning for '$BUCKET'\\"
            \${MC} version enable myminio/$BUCKET
        elif [ $VERSIONING = false ] ; then
            echo \\"Suspending versioning for '$BUCKET'\\"
            \${MC} version suspend myminio/$BUCKET
        fi
      else
          echo \\"Bucket '$BUCKET' versioning unchanged.\\"
      fi

      # At this point, the bucket should exist, skip checking for existence
      # Set policy on the bucket
      echo \\"Setting policy of bucket '$BUCKET' to '$POLICY'.\\"
      \${MC} policy set $POLICY myminio/$BUCKET
    }

    # Try connecting to MinIO instance
    scheme=http
    connectToMinio $scheme


    # Create the buckets
    createBucket session public  
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  labels:
    app: minio
    chart: minio-3.4.3
    heritage: Helm
    release: minio-c8cb4064
  name: minio-c8cb4064
  namespace: default
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: minio
    chart: minio-3.4.3
    heritage: Helm
    release: minio-c8cb4064
  name: minio-c8cb4064-console
  namespace: default
spec:
  ports:
    - name: http
      port: 9001
      protocol: TCP
      targetPort: 9001
  selector:
    app: minio
    release: minio-c8cb4064
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: minio
    chart: minio-3.4.3
    heritage: Helm
    monitoring: \\"true\\"
    release: minio-c8cb4064
  name: minio-c8cb4064
  namespace: default
spec:
  ports:
    - name: http
      nodePort: 32000
      port: 9000
      protocol: TCP
  selector:
    app: minio
    release: minio-c8cb4064
  type: NodePort
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: minio
    chart: minio-3.4.3
    heritage: Helm
    release: minio-c8cb4064
  name: minio-c8cb4064
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: minio
      release: minio-c8cb4064
  strategy:
    rollingUpdate:
      maxSurge: 100%
      maxUnavailable: 0
    type: RollingUpdate
  template:
    metadata:
      annotations:
        checksum/config: 96bb16dd2707eb59b750096ea5f34d78e67a0d4f2a060bd3a2aad50cfb378ee7
        checksum/secrets: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
      labels:
        app: minio
        release: minio-c8cb4064
      name: minio-c8cb4064
    spec:
      containers:
        - command:
            - /bin/sh
            - -ce
            - /usr/bin/docker-entrypoint.sh minio server /export -S /etc/minio/certs/ --address :9000 --console-address :9001
          env:
            - name: MINIO_ROOT_USER
              valueFrom:
                secretKeyRef:
                  key: rootUser
                  name: minio-secret
            - name: MINIO_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: rootPassword
                  name: minio-secret
            - name: MINIO_PROMETHEUS_AUTH_TYPE
              value: public
          image: quay.io/minio/minio:RELEASE.2021-12-20T22-07-16Z
          imagePullPolicy: IfNotPresent
          name: minio
          ports:
            - containerPort: 9000
              name: http
            - containerPort: 9001
              name: http-console
          resources:
            requests:
              memory: 500Mi
          volumeMounts:
            - mountPath: /tmp/credentials
              name: minio-user
              readOnly: true
            - mountPath: /export
              name: export
      securityContext:
        fsGroup: 1000
        runAsGroup: 1000
        runAsUser: 1000
      serviceAccountName: minio-sa
      volumes:
        - name: export
          persistentVolumeClaim:
            claimName: minio-c8cb4064
        - name: minio-user
          secret:
            secretName: minio-secret
---
apiVersion: batch/v1
kind: Job
metadata:
  annotations:
    helm.sh/hook: post-install,post-upgrade
    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation
  labels:
    app: minio-make-bucket-job
    chart: minio-3.4.3
    heritage: Helm
    release: minio-c8cb4064
  name: minio-c8cb4064-make-bucket-job
  namespace: default
spec:
  template:
    metadata:
      labels:
        app: minio-job
        release: minio-c8cb4064
    spec:
      containers:
        - command:
            - /bin/sh
            - /config/initialize
          env:
            - name: MINIO_ENDPOINT
              value: minio-c8cb4064
            - name: MINIO_PORT
              value: \\"9000\\"
          image: quay.io/minio/mc:RELEASE.2021-12-20T23-43-34Z
          imagePullPolicy: IfNotPresent
          name: minio-mc
          resources:
            requests:
              memory: 128Mi
          volumeMounts:
            - mountPath: /config
              name: minio-configuration
      restartPolicy: OnFailure
      volumes:
        - name: minio-configuration
          projected:
            sources:
              - configMap:
                  name: minio-c8cb4064
              - secret:
                  name: minio-secret
---
apiVersion: batch/v1
kind: Job
metadata:
  annotations:
    helm.sh/hook: post-install,post-upgrade
    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation
  labels:
    app: minio-make-user-job
    chart: minio-3.4.3
    heritage: Helm
    release: minio-c8cb4064
  name: minio-c8cb4064-make-user-job
  namespace: default
spec:
  template:
    metadata:
      labels:
        app: minio-job
        release: minio-c8cb4064
    spec:
      containers:
        - command:
            - /bin/sh
            - /config/add-user
          env:
            - name: MINIO_ENDPOINT
              value: minio-c8cb4064
            - name: MINIO_PORT
              value: \\"9000\\"
          image: quay.io/minio/mc:RELEASE.2021-12-20T23-43-34Z
          imagePullPolicy: IfNotPresent
          name: minio-mc
          resources:
            requests:
              memory: 128Mi
          volumeMounts:
            - mountPath: /config
              name: minio-configuration
      restartPolicy: OnFailure
      volumes:
        - name: minio-configuration
          projected:
            sources:
              - configMap:
                  name: minio-c8cb4064
              - secret:
                  name: minio-secret
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: public
  labels:
    app: minio
  name: minio
spec:
  rules:
    - http:
        paths:
          - backend:
              service:
                name: minio-c8cb4064
                port:
                  name: http
            path: /session
            pathType: ImplementationSpecific
"
`;

exports[`NodeServer 1`] = `
"apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: node-server
  name: node-server
spec:
  revisionHistoryLimit: 0
  selector:
    matchLabels:
      app: node-server
  template:
    metadata:
      labels:
        app: node-server
        revision: master-1234568-stable
    spec:
      containers:
        - env:
            - name: NODE_ENV
              value: production
            - name: KIBANA_URL
              value: http://kibana.default:5601
            - name: MONGO_URI
              value: mongodb://mongodb.default.svc/tensorleap?tls=false&ssl=false
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: HOST_NAME
              value: tensorleap.local
            - name: INSECURE_COOKIE
              value: \\"true\\"
          image: gcr.io/tensorleap/node-server:master-1234568-stable
          imagePullPolicy: Always
          name: node-server
          ports:
            - containerPort: 4000
              name: http
      serviceAccountName: node-server-sa
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: node-server
  name: node-server
spec:
  ports:
    - name: http
      port: 80
      targetPort: http
  selector:
    app: node-server
  sessionAffinity: None
  type: NodePort
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: public
  labels:
    app: node-server
  name: node-server
spec:
  rules:
    - http:
        paths:
          - backend:
              service:
                name: node-server
                port:
                  name: http
            path: /api
            pathType: ImplementationSpecific
          - backend:
              service:
                name: node-server
                port:
                  name: http
            path: /kibana
            pathType: ImplementationSpecific
          - backend:
              service:
                name: kibana
                port:
                  number: 5601
            path: /kibana/bundles
            pathType: ImplementationSpecific
          - backend:
              service:
                name: kibana
                port:
                  number: 5601
            path: /kibana/built_assets
            pathType: ImplementationSpecific
          - backend:
              service:
                name: node-server
                port:
                  name: http
            path: /socket.io
            pathType: ImplementationSpecific
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: node-server-mongodb-c84a1b14
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mongodb
    helm.sh/chart: mongodb-9.3.1
  name: mongodb
  namespace: default
secrets:
  - name: mongodb
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  labels:
    app.kubernetes.io/component: mongodb
    app.kubernetes.io/instance: node-server-mongodb-c84a1b14
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mongodb
    helm.sh/chart: mongodb-9.3.1
  name: mongodb
  namespace: default
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 8Gi
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: mongodb
    app.kubernetes.io/instance: node-server-mongodb-c84a1b14
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mongodb
    helm.sh/chart: mongodb-9.3.1
  name: mongodb
  namespace: default
spec:
  ports:
    - name: mongodb
      port: 27017
      targetPort: mongodb
  selector:
    app.kubernetes.io/component: mongodb
    app.kubernetes.io/instance: node-server-mongodb-c84a1b14
    app.kubernetes.io/name: mongodb
  type: ClusterIP
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/component: mongodb
    app.kubernetes.io/instance: node-server-mongodb-c84a1b14
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mongodb
    helm.sh/chart: mongodb-9.3.1
  name: mongodb
  namespace: default
spec:
  selector:
    matchLabels:
      app.kubernetes.io/component: mongodb
      app.kubernetes.io/instance: node-server-mongodb-c84a1b14
      app.kubernetes.io/name: mongodb
  strategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/component: mongodb
        app.kubernetes.io/instance: node-server-mongodb-c84a1b14
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: mongodb
        helm.sh/chart: mongodb-9.3.1
    spec:
      containers:
        - env:
            - name: BITNAMI_DEBUG
              value: \\"false\\"
            - name: ALLOW_EMPTY_PASSWORD
              value: \\"yes\\"
            - name: MONGODB_SYSTEM_LOG_VERBOSITY
              value: \\"0\\"
            - name: MONGODB_DISABLE_SYSTEM_LOG
              value: \\"no\\"
            - name: MONGODB_ENABLE_IPV6
              value: \\"no\\"
            - name: MONGODB_ENABLE_DIRECTORY_PER_DB
              value: \\"no\\"
          image: docker.io/bitnami/mongodb:4.4.1-debian-10-r39
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
                - mongo
                - --eval
                - db.adminCommand('ping')
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: mongodb
          ports:
            - containerPort: 27017
              name: mongodb
          readinessProbe:
            exec:
              command:
                - mongo
                - --eval
                - db.adminCommand('ping')
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits: {}
            requests: {}
          securityContext:
            runAsNonRoot: true
            runAsUser: 1001
          volumeMounts:
            - mountPath: /bitnami/mongodb
              name: datadir
      securityContext:
        fsGroup: 1001
        sysctls: []
      serviceAccountName: mongodb
      volumes:
        - name: datadir
          persistentVolumeClaim:
            claimName: mongodb
---
apiVersion: v1
kind: ServiceAccount
imagePullSecrets:
  - name: gcr-access-token
metadata:
  name: node-server-sa
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: node-server-sa-role
rules:
  - apiGroups:
      - batch
    resources:
      - jobs
    verbs:
      - list
      - get
      - create
      - delete
  - apiGroups:
      - \\"\\"
    resources:
      - configmaps
    verbs:
      - get
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: node-server-sa-role-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: node-server-sa-role
subjects:
  - kind: ServiceAccount
    name: node-server-sa
    namespace: default
"
`;

exports[`RabbitMQ 1`] = `
"apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app: rabbitmq-master
    app.kubernetes.io/instance: rabbitmq-c817e59a
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: rabbitmq
    helm.sh/chart: rabbitmq-8.24.3
  name: rabbitmq-c817e59a
  namespace: default
automountServiceAccountToken: true
secrets:
  - name: rabbitmq-c817e59a
---
apiVersion: v1
kind: Secret
metadata:
  labels:
    app: rabbitmq-master
    app.kubernetes.io/instance: rabbitmq-c817e59a
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: rabbitmq
    helm.sh/chart: rabbitmq-8.24.3
  name: rabbitmq-c817e59a
  namespace: default
data:
  rabbitmq-erlang-cookie: M2UzYWJhZTItNjMyNS0xMWVjLTkwZDYtMDI0MmFjMTIwMDAz
  rabbitmq-password: M2UzYWJhZTItNjMyNS0xMWVjLTkwZDYtMDI0MmFjMTIwMDAz
type: Opaque
---
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    app: rabbitmq-master
    app.kubernetes.io/instance: rabbitmq-c817e59a
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: rabbitmq
    helm.sh/chart: rabbitmq-8.24.3
  name: rabbitmq-c817e59a-config
  namespace: default
data:
  rabbitmq.conf: |-
    ## Username and password
    ##
    default_user = user
    default_pass = CHANGEME
    ## Clustering
    ##
    cluster_formation.peer_discovery_backend  = rabbit_peer_discovery_k8s
    cluster_formation.k8s.host = kubernetes.default.svc.cluster.local
    cluster_formation.node_cleanup.interval = 10
    cluster_formation.node_cleanup.only_log_warning = true
    cluster_partition_handling = autoheal
    # queue master locator
    queue_master_locator = min-masters
    # enable guest user
    loopback_users.guest = false
    #default_vhost = default-vhost
    #disk_free_limit.absolute = 50MB
    #load_definitions = /app/load_definition.json
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app: rabbitmq-master
    app.kubernetes.io/instance: rabbitmq-c817e59a
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: rabbitmq
    helm.sh/chart: rabbitmq-8.24.3
  name: rabbitmq-c817e59a-endpoint-reader
  namespace: default
rules:
  - apiGroups:
      - \\"\\"
    resources:
      - endpoints
    verbs:
      - get
  - apiGroups:
      - \\"\\"
    resources:
      - events
    verbs:
      - create
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app: rabbitmq-master
    app.kubernetes.io/instance: rabbitmq-c817e59a
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: rabbitmq
    helm.sh/chart: rabbitmq-8.24.3
  name: rabbitmq-c817e59a-endpoint-reader
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: rabbitmq-c817e59a-endpoint-reader
subjects:
  - kind: ServiceAccount
    name: rabbitmq-c817e59a
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: rabbitmq-master
    app.kubernetes.io/instance: rabbitmq-c817e59a
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: rabbitmq
    helm.sh/chart: rabbitmq-8.24.3
  name: rabbitmq-c817e59a-headless
  namespace: default
spec:
  clusterIP: None
  ports:
    - name: epmd
      port: 4369
      targetPort: epmd
    - name: amqp
      port: 5672
      targetPort: amqp
    - name: dist
      port: 25672
      targetPort: dist
    - name: http-stats
      port: 15672
      targetPort: stats
  selector:
    app.kubernetes.io/instance: rabbitmq-c817e59a
    app.kubernetes.io/name: rabbitmq
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: rabbitmq-master
    app.kubernetes.io/instance: rabbitmq-c817e59a
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: rabbitmq
    helm.sh/chart: rabbitmq-8.24.3
  name: rabbitmq-c817e59a
  namespace: default
spec:
  ports:
    - name: amqp
      port: 5672
      targetPort: amqp
    - name: epmd
      port: 4369
      targetPort: epmd
    - name: dist
      port: 25672
      targetPort: dist
    - name: http-stats
      port: 15672
      targetPort: stats
  selector:
    app.kubernetes.io/instance: rabbitmq-c817e59a
    app.kubernetes.io/name: rabbitmq
  type: ClusterIP
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app: rabbitmq-master
    app.kubernetes.io/instance: rabbitmq-c817e59a
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: rabbitmq
    helm.sh/chart: rabbitmq-8.24.3
  name: rabbitmq-c817e59a
  namespace: default
spec:
  podManagementPolicy: OrderedReady
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: rabbitmq-c817e59a
      app.kubernetes.io/name: rabbitmq
  serviceName: rabbitmq-c817e59a-headless
  template:
    metadata:
      annotations:
        checksum/config: 114a3eead1cd6ba07d83e8f52be19336811d40175e949d5d0972cd2a6cea1dd6
        checksum/secret: ee941c52ed6b0f6b9e7d414f646fbbe410a0f7c409e7726791ea3cadf3fc7918
      labels:
        app.kubernetes.io/instance: rabbitmq-c817e59a
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: rabbitmq
        helm.sh/chart: rabbitmq-8.24.3
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: rabbitmq-c817e59a
                    app.kubernetes.io/name: rabbitmq
                namespaces:
                  - default
                topologyKey: kubernetes.io/hostname
              weight: 1
      containers:
        - env:
            - name: BITNAMI_DEBUG
              value: \\"false\\"
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: MY_POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: K8S_SERVICE_NAME
              value: rabbitmq-c817e59a-headless
            - name: K8S_ADDRESS_TYPE
              value: hostname
            - name: RABBITMQ_FORCE_BOOT
              value: \\"no\\"
            - name: RABBITMQ_NODE_NAME
              value: rabbit@$(MY_POD_NAME).$(K8S_SERVICE_NAME).$(MY_POD_NAMESPACE).svc.cluster.local
            - name: K8S_HOSTNAME_SUFFIX
              value: .$(K8S_SERVICE_NAME).$(MY_POD_NAMESPACE).svc.cluster.local
            - name: RABBITMQ_MNESIA_DIR
              value: /bitnami/rabbitmq/mnesia/$(RABBITMQ_NODE_NAME)
            - name: RABBITMQ_LDAP_ENABLE
              value: \\"no\\"
            - name: RABBITMQ_LOGS
              value: \\"-\\"
            - name: RABBITMQ_ULIMIT_NOFILES
              value: \\"65536\\"
            - name: RABBITMQ_USE_LONGNAME
              value: \\"true\\"
            - name: RABBITMQ_ERL_COOKIE
              valueFrom:
                secretKeyRef:
                  key: rabbitmq-erlang-cookie
                  name: rabbitmq-c817e59a
            - name: RABBITMQ_LOAD_DEFINITIONS
              value: \\"no\\"
            - name: RABBITMQ_SECURE_PASSWORD
              value: \\"yes\\"
            - name: RABBITMQ_USERNAME
              value: user
            - name: RABBITMQ_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: rabbitmq-password
                  name: rabbitmq-c817e59a
            - name: RABBITMQ_PLUGINS
              value: rabbitmq_management, rabbitmq_peer_discovery_k8s, rabbitmq_auth_backend_ldap
          image: docker.io/bitnami/rabbitmq:3.9.9-debian-10-r0
          imagePullPolicy: IfNotPresent
          lifecycle:
            preStop:
              exec:
                command:
                  - /bin/bash
                  - -ec
                  - |
                    if [[ -f /opt/bitnami/scripts/rabbitmq/nodeshutdown.sh ]]; then
                        /opt/bitnami/scripts/rabbitmq/nodeshutdown.sh -t \\"120\\" -d  \\"false\\"
                    else
                        rabbitmqctl stop_app
                    fi
          livenessProbe:
            exec:
              command:
                - /bin/bash
                - -ec
                - rabbitmq-diagnostics -q ping
            failureThreshold: 6
            initialDelaySeconds: 120
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 20
          name: rabbitmq
          ports:
            - containerPort: 5672
              name: amqp
            - containerPort: 25672
              name: dist
            - containerPort: 15672
              name: stats
            - containerPort: 4369
              name: epmd
          readinessProbe:
            exec:
              command:
                - /bin/bash
                - -ec
                - rabbitmq-diagnostics -q check_running && rabbitmq-diagnostics -q check_local_alarms
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 20
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - mountPath: /bitnami/rabbitmq/conf
              name: configuration
            - mountPath: /bitnami/rabbitmq/mnesia
              name: data
      securityContext:
        fsGroup: 1001
        runAsUser: 1001
      serviceAccountName: rabbitmq-c817e59a
      terminationGracePeriodSeconds: 120
      volumes:
        - configMap:
            items:
              - key: rabbitmq.conf
                path: rabbitmq.conf
            name: rabbitmq-c817e59a-config
          name: configuration
  updateStrategy:
    type: RollingUpdate
  volumeClaimTemplates:
    - metadata:
        labels:
          app.kubernetes.io/instance: rabbitmq-c817e59a
          app.kubernetes.io/name: rabbitmq
        name: data
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 8Gi
"
`;

exports[`Web UI 1`] = `
"apiVersion: v1
kind: ServiceAccount
imagePullSecrets:
  - name: gcr-access-token
metadata:
  labels:
    app: web-ui
  name: web-ui-sa
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: web-ui
  name: tensorleap-web-ui
spec:
  revisionHistoryLimit: 0
  selector:
    matchLabels:
      app: web-ui
  template:
    metadata:
      labels:
        app: web-ui
        revision: master-1234568-stable
    spec:
      containers:
        - env:
            - name: NODE_ENV
              value: production
          image: gcr.io/tensorleap/web-ui:master-1234568-stable
          imagePullPolicy: Always
          name: web-ui
          ports:
            - containerPort: 8080
              name: http
      serviceAccountName: web-ui-sa
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: web-ui
  name: tensorleap-web-ui
spec:
  ports:
    - name: http
      port: 8080
  selector:
    app: web-ui
  sessionAffinity: None
  type: NodePort
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: public
  labels:
    app: web-ui
  name: tensorleap-web-ui
spec:
  defaultBackend:
    service:
      name: tensorleap-web-ui
      port:
        name: http
"
`;
